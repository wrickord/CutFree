{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python2.7/dist-packages (1.13.3)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-26d84fe4e5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# data analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m pip install numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# system\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "!pip install transformers\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# custom\n",
    "dir_path = \"/content/drive/MyDrive/00-Colab_Notebooks/CutFreeGPT/\"\n",
    "sys.path.append(dir_path)\n",
    "from CheckRandomer import check_randomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Directory Path to Save Model Information\n",
    "# check for models directory\n",
    "if not os.path.exists(\"tf-models\"):\n",
    "    os.mkdir(\"tf-models\")\n",
    "\n",
    "# get previous model version\n",
    "highest_model_number = 0\n",
    "for directory in os.listdir(\"tf-models\"):\n",
    "    model_number = int(directory.split(\"-V\")[-1])\n",
    "    if model_number >= highest_model_number:\n",
    "        highest_model_number = model_number + 1\n",
    "\n",
    "# create save directory\n",
    "save_folder = \"tf-models/AlgorithmClassifier-V\" + str(highest_model_number)\n",
    "os.mkdir(save_folder)\n",
    "\n",
    "# create other subdirectories\n",
    "os.mkdir(save_folder + \"/checkpoints\")\n",
    "os.mkdir(save_folder + \"/logs\")\n",
    "os.mkdir(save_folder + \"/plots\")\n",
    "os.mkdir(save_folder + \"/results\")\n",
    "os.mkdir(save_folder + \"/class_weights\")\n",
    "\n",
    "# get random state to improve validity of results\n",
    "RANDOM_STATE = np.random.randint(0, 1000)\n",
    "print(\"Random State: \", RANDOM_STATE)\n",
    "\n",
    "# read in data to dataframe\n",
    "file_path = dir_path + \"cutfree_data.csv\"\n",
    "df_gpt = pd.read_csv(file_path)\n",
    "\n",
    "# shuffle dataframe\n",
    "df_gpt = df_gpt.sample(\n",
    "    frac=1,\n",
    "    random_state=RANDOM_STATE\n",
    ").reset_index(drop=True).drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df_gpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function constants\n",
    "IUB_CODES = {\n",
    "    \"A\": np.array([\"A\"]),\n",
    "    \"C\": np.array([\"C\"]),\n",
    "    \"G\": np.array([\"G\"]),\n",
    "    \"T\": np.array([\"T\"]),\n",
    "    \"R\": np.array([\"A\", \"G\"]),\n",
    "    \"Y\": np.array([\"C\", \"T\"]),\n",
    "    \"S\": np.array([\"C\", \"G\"]),\n",
    "    \"W\": np.array([\"A\", \"T\"]),\n",
    "    \"K\": np.array([\"G\", \"T\"]),\n",
    "    \"M\": np.array([\"A\", \"C\"]),\n",
    "    \"B\": np.array([\"C\", \"G\", \"T\"]),\n",
    "    \"D\": np.array([\"A\", \"G\", \"T\"]),\n",
    "    \"H\": np.array([\"A\", \"C\", \"T\"]),\n",
    "    \"V\": np.array([\"A\", \"C\", \"G\"]),\n",
    "    \"N\": np.array([\"A\", \"C\", \"G\", \"T\"])\n",
    "}\n",
    "\n",
    "\n",
    "# function to expand one-hot encoding to include all valid subcodes\n",
    "def get_subcodes(output):\n",
    "    # fix output to array\n",
    "    arr = np.array([float(label) for label in output[1:-2].split(\" \")])\n",
    "\n",
    "    # get subcode values\n",
    "    idx = np.where(np.array(arr) == 1)[0][0]\n",
    "    val = list(IUB_CODES.keys())[idx]\n",
    "\n",
    "    # get subcodes\n",
    "    codes = \"\"\n",
    "    for key, value in IUB_CODES.items():\n",
    "        if set(value).issubset(set(IUB_CODES[val])):\n",
    "            codes += key\n",
    "    codes = list(codes)\n",
    "\n",
    "    # update encoding from subcodes\n",
    "    for code in codes:\n",
    "        idx = list(IUB_CODES.keys()).index(code)\n",
    "        arr[idx] = 1\n",
    "\n",
    "    # return subcodes\n",
    "    return arr\n",
    "\n",
    "\n",
    "# gather inputs and outputs for finetuning\n",
    "inputs = []\n",
    "outputs = []\n",
    "for index, row in df_gpt.iterrows():\n",
    "    # input texts\n",
    "    oligo = row[\"Oligo\"]\n",
    "    sites = row[\"Sites\"][1:-1].replace(\"'\", \"\")\n",
    "    input_randomer = str(row[\"Input_Randomer\"])\n",
    "    inputs.append((\"; \").join([oligo, sites, input_randomer]))\n",
    "\n",
    "    # output texts\n",
    "    output = [float(label) for label in row[\"Output\"][1:-2].split(\" \")]\n",
    "    outputs.append(output)\n",
    "\n",
    "# train and val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# model\n",
    "class CutFreeGPT(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super(CutFreeGPT, self).__init__()\n",
    "\n",
    "        self.base_model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(64, 15)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.base_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        ).hidden_states\n",
    "        cls_embeddings = hidden_states[-1][:, 0, :]\n",
    "        mlp_out = self.mlp(cls_embeddings)\n",
    "\n",
    "        return self.output_layer(mlp_out)\n",
    "\n",
    "\n",
    "def encode_data(data, tokenizer=TOKENIZER):\n",
    "    tokens = tokenizer(\n",
    "        data,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# instantiate model, loss function, and optimizer\n",
    "model = CutFreeGPT()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# tokenize and create datasets\n",
    "input_ids_train, attention_mask_train = encode_data(x_train)\n",
    "input_ids_val, attention_mask_val = encode_data(x_val)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids_train,\n",
    "    attention_mask_train,\n",
    "    torch.tensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    input_ids_val,\n",
    "    attention_mask_val,\n",
    "    torch.tensor(y_val)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# fine-tuning loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss += loss_function(outputs, labels).item()\n",
    "            preds = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += preds.eq(labels.view_as(preds)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}, \" + \\\n",
    "        f\"Val loss: {val_loss:.4f}, \" + \\\n",
    "        f\"Val Accuracy: {correct/len(val_loader.dataset):.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
