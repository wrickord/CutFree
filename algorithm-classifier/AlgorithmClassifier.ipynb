{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the Most Efficient Method for a Biological Optimization Problem using Deep Bidirectional Encoder Representations from Transformers (BERT)\n",
    "Designed to automatically select between deterministic and heuristic methods of solving a biological optimization problem, CutFree, using the BERT unsupervised model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, AdamWeightDecay\n",
    "from numba import cuda\n",
    "\n",
    "# check device\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Directory Path to Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get previous model version\n",
    "highest_model_number = 0\n",
    "for directory in os.listdir('models'):\n",
    "    model_number = int(directory.split(\"-V\")[-1])\n",
    "    if model_number >= highest_model_number:\n",
    "        highest_model_number = model_number + 1\n",
    "\n",
    "# create save directory\n",
    "save_folder = 'models/AlgorithmClassifier-V' + str(highest_model_number)\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset to Include Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data to dataframe\n",
    "file_path = '../runtime-simulations/runtime_data.csv'\n",
    "df_original = pd.read_csv(file_path)\n",
    "df = df_original.copy()\n",
    "\n",
    "# drop duplicate rows\n",
    "duplicate_rows = df.duplicated(subset=['Oligo', 'Sites'], keep='first')\n",
    "df = df[~duplicate_rows]\n",
    "\n",
    "# fix the sites column\n",
    "df['Sites'] = [s[5:-2] for s in df['Sites']]\n",
    "df['Sites'] = [s.replace('\"', '').replace(' ', '') for s in df['Sites']]\n",
    "df['Sites'] = [s.split(',') for s in df['Sites']]\n",
    "\n",
    "# add time discrepancy column\n",
    "df['Discrepancy'] = df['CutFree_Time'] - df['CutFreeRL_Time']\n",
    "df = df.sort_values(by=['Discrepancy'], ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# add correct algorithm column\n",
    "conditions = [\n",
    "    (df['CutFree_Time'] <= df['CutFreeRL_Time']),\n",
    "    (df['CutFree_Time'] > df['CutFreeRL_Time'])\n",
    "]\n",
    "values = [0, 1] # 0 = CutFree, 1 = CutFreeRL\n",
    "df['Algorithm'] = np.select(conditions, values)\n",
    "\n",
    "# adjust correct algorithm based on degeneracy if it outside of the confidence interval \n",
    "# (i.e., ignore CutFreeRL if the degeneracy loss is too significant, typically caused by incomplete CutFreeRL output)\n",
    "df.loc[df['CutFree_Degeneracy'] == 0, 'Algorithm'] = 1\n",
    "df.loc[df['CutFreeRL_Degeneracy'] <= df['CutFree_Degeneracy'] - (df['CutFree_Degeneracy'] * 0.10), 'Algorithm'] = 0\n",
    "\n",
    "# count classifcations\n",
    "print(df['Algorithm'].value_counts())\n",
    "\n",
    "df.shape, df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Train, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather text and label information\n",
    "texts = []\n",
    "labels = []\n",
    "for index, row in df.iterrows():\n",
    "    texts.append(row['Oligo'] + '; ' + ', '.join(row['Sites']))\n",
    "    labels.append(row['Algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random state to improve validity of results\n",
    "random_state = np.random.randint(0, 1000)\n",
    "\n",
    "# load train, validation, and test data for text\n",
    "x_train, x_val, y_train, y_val = train_test_split(texts, labels, test_size=0.3, random_state=random_state)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, random_state=random_state)\n",
    "\n",
    "print(\"Random State: \", random_state)\n",
    "print(\"Train, Validation, and Test Sizes: \", len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Input Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "BATCH_SIZE = 24\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# tokenize training data\n",
    "tokens_train = tokenizer.batch_encode_plus(x_train, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n",
    "\n",
    "input_ids_train = tokens_train['input_ids']\n",
    "attention_mask_train = tokens_train['attention_mask']\n",
    "\n",
    "texts_train = (input_ids_train, attention_mask_train)\n",
    "labels_train = tf.constant(y_train)\n",
    "\n",
    "# tokenize validation data\n",
    "tokens_val = tokenizer.batch_encode_plus(x_val, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n",
    "\n",
    "input_ids_val = tokens_val['input_ids']\n",
    "attention_mask_val = tokens_val['attention_mask']\n",
    "\n",
    "texts_val = (input_ids_val, attention_mask_val)\n",
    "labels_val = tf.constant(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train, and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# bert model\n",
    "bert_model = TFDistilBertModel.from_pretrained(BERT_MODEL_NAME, output_attentions=False, output_hidden_states=False)([input_ids, attention_mask])\n",
    "bert_output = bert_model.last_hidden_state[:, 0, :]\n",
    "\n",
    "# mlp layers\n",
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dropout(0.8, name='dropout_0'),\n",
    "    tf.keras.layers.Dense(768, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_1'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_1'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_1'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_1'),\n",
    "    tf.keras.layers.Dense(512, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_2'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_2'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_2'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_2'),\n",
    "    tf.keras.layers.Dense(256, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_3'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_3'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_3'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_3'),\n",
    "    tf.keras.layers.Dense(128, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_4'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_4'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_4'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_4'),\n",
    "    tf.keras.layers.Dense(64, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_5'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_5'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_5'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_5'),\n",
    "    tf.keras.layers.Dense(32, \n",
    "                          kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          bias_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          activity_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "                          name='dense_6'),\n",
    "    tf.keras.layers.BatchNormalization(name='batch_norm_6'),\n",
    "    tf.keras.layers.Dropout(0.4, name='dropout_6'),\n",
    "    tf.keras.layers.Activation('relu', name='relu_6')\n",
    "], name='mlp')(bert_output)\n",
    "\n",
    "# output layer\n",
    "output = tf.keras.layers.Dense(2, activation='softmax', name='output')(mlp)\n",
    "\n",
    "# combine input and output layers to create model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output, name='AlgorithmClassifier')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer and loss function\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-6, weight_decay_rate=0.01)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer, loss, metrics=['accuracy'])\n",
    "\n",
    "# class weights (if not balanced)\n",
    "class_weights = dict(enumerate(compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=np.array(y_train))))\n",
    "\n",
    "# callbacks for early stopping, saving checkpoints, and visualizing in tensorboard\n",
    "model_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=save_folder + '/checkpoints/model.{epoch:02d}-{val_accuracy:.3f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=save_folder + '/logs')]\n",
    "\n",
    "# fine-tune model to training data\n",
    "history = model.fit(\n",
    "    texts_train,\n",
    "    labels_train,\n",
    "    validation_data=(texts_val, labels_val), \n",
    "    epochs=50, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_folder + '/tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(save_folder + '/training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize test data\n",
    "tokens_test = tokenizer.batch_encode_plus(x_test, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n",
    "\n",
    "input_ids_test = tokens_test['input_ids']\n",
    "attention_mask_test = tokens_test['attention_mask']\n",
    "\n",
    "texts_test = (input_ids_test, attention_mask_test)\n",
    "labels_test = tf.constant(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get accuracy of model predictions\n",
    "def get_accuracy(pred, true):\n",
    "    test_accuracy = accuracy_score(pred, true) * 100\n",
    "    print('Accuracy: {:.2f}%'.format(test_accuracy))\n",
    "    return test_accuracy\n",
    "\n",
    "# retrieve model predictions on test data\n",
    "y_pred = model.predict(texts_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# get accuracy of predictions\n",
    "acc = get_accuracy(y_pred, y_test)\n",
    "\n",
    "# save accuracy\n",
    "with open(save_folder + \"/accuracy_results.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([acc])\n",
    "    \n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classification report\n",
    "class_report = classification_report(y_pred, y_test, target_names=['CutFree', 'CutFreeRL'])\n",
    "\n",
    "# save classification report\n",
    "with open(save_folder + \"/classification_report.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([class_report])\n",
    "    \n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=['CutFree', 'CutFreeRL'])\n",
    "fig.plot()\n",
    "\n",
    "# save confusion matrix\n",
    "plt.savefig(save_folder + '/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Correct Selection for CutFree and CutFreeRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors and font for plots\n",
    "rgb = []\n",
    "for _ in df[df['Algorithm'] == 0].index:\n",
    "    c = [46/255, 108/255, 190/255]\n",
    "    rgb.append(c)\n",
    "\n",
    "rgb2 = []\n",
    "for _ in df[df['Algorithm'] == 1].index:\n",
    "    c = [220/255, 77/255, 58/255]\n",
    "    rgb2.append(c)\n",
    "\n",
    "gfont = {'fontname': 'Georgia'}\n",
    "font = font_manager.FontProperties(family='Georgia', style='normal', size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# plot expected algorithm selection for all time discrepencies\n",
    "plt.scatter(\n",
    "    df[df['Algorithm'] == 0].index, df[df['Algorithm'] == 0]['Discrepancy'], \n",
    "    c=rgb,\n",
    "    linewidths=1,\n",
    "    marker='|',\n",
    "    s=500)\n",
    "plt.scatter(\n",
    "    df[df['Algorithm'] == 1].index, df[df['Algorithm'] == 1]['Discrepancy'], \n",
    "    c=rgb2,\n",
    "    linewidths=1,\n",
    "    marker='_',\n",
    "    s=500)\n",
    "    \n",
    "plt.title('Correct Algorithm Selection for All Runtime Discrepancies', fontsize=32, **gfont)\n",
    "plt.ylabel('Runime Discrepancy (sec)', fontsize=32, **gfont)\n",
    "plt.legend(['CutFree', 'CutFreeRL'], prop=font)\n",
    "ax = plt.gca()\n",
    "plt.xticks(fontsize=32, **gfont)\n",
    "plt.yticks(fontsize=32, **gfont)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "plt.savefig(save_folder + '/correct_algorithm_selection.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Predictions for Selection of CutFree and CutFreeRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize test data for analysis\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# get oligos from test inputs\n",
    "x_test_oligos = [i.split(';')[0] for i in x_test]\n",
    "\n",
    "# get sites from test inputs\n",
    "x_test_sites = [i.split(';')[1] for i in x_test]\n",
    "x_test_sites = np.array(x_test_sites)\n",
    "x_test_sites = [x_test_site.replace(' ', '') for x_test_site in x_test_sites]\n",
    "x_test_sites = [x_test_site.split(',') for x_test_site in x_test_sites]\n",
    "\n",
    "print(len(x_test_oligos), len(x_test_sites), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataframe\n",
    "df_test = df.copy()\n",
    "\n",
    "# add prediction column using test data\n",
    "for oligo, site, pred, test in zip(x_test_oligos, x_test_sites, y_pred, y_test):\n",
    "    for index, (oligo_df, sites_df) in enumerate(zip(df_test['Oligo'].values, df_test['Sites'].values)):\n",
    "        if (oligo == oligo_df) and (site == sites_df):\n",
    "            df_test.loc[index, 'Prediction'] = pred == test\n",
    "\n",
    "# drop unassigned rows\n",
    "df_test = df_test.dropna(subset=['Prediction'])\n",
    "\n",
    "# set prediction algorithm\n",
    "cutfree_condition = ((df_test['Prediction'] == True) & (df_test['Algorithm'] == 0)) | ((df_test['Prediction'] == False) & (df_test['Algorithm'] == 1))\n",
    "cutfreerl_condition = ((df_test['Prediction'] == True) & (df_test['Algorithm'] == 1)) | ((df_test['Prediction'] == False) & (df_test['Algorithm'] == 0))\n",
    "df_test.loc[cutfree_condition, 'Prediction'] = 0\n",
    "df_test.loc[cutfreerl_condition, 'Prediction'] = 1\n",
    "\n",
    "# create correct/incorrect column\n",
    "df_test['Correct'] = df_test['Algorithm'] == df_test['Prediction']\n",
    "\n",
    "# sort dataframe by discrepency and reset index\n",
    "df_test = df_test.sort_values(by=['Discrepancy'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "# check prediction column\n",
    "df_test['Prediction'].value_counts() # 0 = CutFree, 1 = CutFreeRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 16))\n",
    "\n",
    "# plot correct and incorrect selections for all time discrepencies\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "correct_conditions = (df_test['Correct'] == True)\n",
    "plt.scatter(\n",
    "    df_test[correct_conditions].index,\n",
    "    df_test.loc[correct_conditions, 'Discrepancy'], \n",
    "    c='g',\n",
    "    linewidths=1,\n",
    "    marker='s',\n",
    "    s=50)\n",
    "\n",
    "incorrect_conditions = (df_test['Correct'] == False)\n",
    "plt.scatter(\n",
    "    df_test[incorrect_conditions].index,\n",
    "    df_test.loc[incorrect_conditions, 'Discrepancy'], \n",
    "    c='r',\n",
    "    linewidths=1,\n",
    "    marker='x',\n",
    "    s=500)\n",
    "\n",
    "plt.title(f'Accuracy of BERT-based Neural Network ({acc:.1f}%) for All Runtime Discrepancies', fontsize=32, **gfont)\n",
    "plt.ylabel('Runime Discrepancy (sec)', fontsize=32, **gfont)\n",
    "plt.legend([f'Correct Predictions ({len(df_test[correct_conditions])})', \n",
    "            f'Incorrect Predictions ({len(df_test[incorrect_conditions])})'], \n",
    "            prop=font)\n",
    "ax = plt.gca()\n",
    "plt.xticks(fontsize=32, **gfont)\n",
    "plt.yticks(fontsize=32, **gfont)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "# plot problematic selections for all time discrepencies\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "nonproblematic_conditions = (df_test['Correct'] == True) | ((df_test['Correct'] == False) & (df_test['Algorithm'] == 0)) | ((df_test['Correct'] == False) & (df_test['CutFree_Time'] <= 60))\n",
    "plt.scatter(\n",
    "    df_test[nonproblematic_conditions].index,\n",
    "    df_test.loc[nonproblematic_conditions, 'Discrepancy'], \n",
    "    c='g',\n",
    "    linewidths=1,\n",
    "    marker='s',\n",
    "    s=50)\n",
    "\n",
    "# problematic conditions are defined as incorrect predictions that place the system at risk of runtime explosion\n",
    "problematic_conditions = ((df_test['Correct'] == False) & (df_test['Algorithm'] == 1) & (df_test['CutFree_Time'] >= 60))\n",
    "plt.scatter(\n",
    "    df_test[problematic_conditions].index,\n",
    "    df_test.loc[problematic_conditions, 'Discrepancy'], \n",
    "    c='r',\n",
    "    linewidths=1,\n",
    "    marker='x',\n",
    "    s=500)\n",
    "\n",
    "plt.title(f'Non-problematic vs. Problematic Predictions for All Runtime Discrepancies', fontsize=32, **gfont)\n",
    "plt.ylabel('Runime Discrepancy (sec)', fontsize=32, **gfont)\n",
    "plt.legend([f'Non-problematic Predictions ({len(df_test[nonproblematic_conditions])})', \n",
    "            f'Problematic Predictions ({len(df_test[problematic_conditions])})'], \n",
    "            prop=font)\n",
    "ax = plt.gca()\n",
    "plt.xticks(fontsize=32, **gfont)\n",
    "plt.yticks(fontsize=32, **gfont)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "plt.savefig(save_folder + '/runtime_discrepancies.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sensitivity for values above runtime limit\n",
    "sensitivity = (len(df_test[nonproblematic_conditions])) / (len(df_test[nonproblematic_conditions]) + len(df_test[problematic_conditions])) * 100\n",
    "\n",
    "# save sensitivity\n",
    "with open(save_folder + \"/sensitivity_results.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([sensitivity])\n",
    "    \n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Average Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values\n",
    "average_sensitivity = np.array([])\n",
    "average_acc = np.array([])\n",
    "\n",
    "# retrieve all values from saved CSV files\n",
    "for directory in os.listdir('models'):\n",
    "    if directory.startswith('AlgorithmClassifier'):\n",
    "        with open('models/' + directory + '/sensitivity_results.csv', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if len(row) > 0:\n",
    "                    average_sensitivity = np.append(average_sensitivity, float(row[0]))\n",
    "        with open('models/' + directory + '/accuracy_results.csv', 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if len(row) > 0:\n",
    "                    average_acc = np.append(average_acc, float(row[0]))\n",
    "        n = len(average_sensitivity)\n",
    "\n",
    "print(f\"Average Sensitivity: {average_sensitivity.mean():.1f}% +/- {average_sensitivity.std():.5f}\")\n",
    "print(f\"Average Accuracy: {average_acc.mean():.1f}% +/- {average_acc.std():.5f}\")\n",
    "print(f\"Number of Trials: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Make New Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, AdamWeightDecay\n",
    "\n",
    "# load model\n",
    "new_model = tf.keras.models.load_model(\n",
    "    'models/AlgorithmClassifier-V3/checkpoints/model.25-0.909.h5',\n",
    "    custom_objects={'TFDistilBertModel': TFDistilBertModel, \n",
    "                    'AdamWeightDecay': AdamWeightDecay})\n",
    "\n",
    "# load tokenizer\n",
    "new_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input\n",
    "text = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN; ACGWGN, TTTCGGCC, RTAGGCAY, CCTGCATAGG'\n",
    "text_0 = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN; GCGGCCGC, GGCCGGCC, RTGCGCAY, CCTCGAGG'\n",
    "text_1 = 'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN; CRCCGGYG, CCTCGAGG, GTTTAAAC, ATTTAAAT, CGCGCGCG, GGCGCGCC, RTGCGCAY, TTAATTAA, CGTCGACG, GCCCGGGC'\n",
    "\n",
    "new_tokens_test = new_tokenizer.batch_encode_plus([text, text_0, text_1], max_length=256, padding='max_length', truncation=True, return_tensors='tf')\n",
    "\n",
    "new_input_ids_test = new_tokens_test['input_ids']\n",
    "new_attention_mask_test = new_tokens_test['attention_mask']\n",
    "\n",
    "new_texts_test = (new_input_ids_test, new_attention_mask_test)\n",
    "\n",
    "# predict\n",
    "new_y_pred = np.argmax(new_model.predict(new_texts_test), axis=1)\n",
    "\n",
    "print('Reconstructed Model Prediction (0 = CutFree, 1 = CutFreeRL): ', new_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
