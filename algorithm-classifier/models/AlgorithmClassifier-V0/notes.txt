This experiment was recorded with all model architectures using L1L2
regularization with a value of 1e-3.

Additionally, the dropout value for the first layer after BERT was 0.5.

Previous implementation of the model with L1L2 = 1e-2 proved successful,
with BERT getting an accuracy of 90.71% and the MLP getting 72.24% accuracy.

An explanation of L1L2 regularization for future reference. Both techniques
are methods of reducing overfitting in a model by adjusting the loss.

L1 (Lasso):
- adds abs(weights) to loss
- encourages sparsity
- useful for feature selection
- typically range between 1e-1 to 1e-3

L2 (Ridge):
- adds sqrd(weights) to loss
- encourages small weights
- builds simpler models, useful for codependent features
- typically range between 1e-1 to 1e-3
